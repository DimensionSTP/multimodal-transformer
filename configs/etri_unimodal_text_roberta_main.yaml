# @package _global_
defaults:
  - dataset_module: unimodal_text_kemdy_dataset
  - training_arguments: unimodal_training_arguments
  - hydra: hydra

project_dir: /home/ddang/multimodal-transformer

seed: 2024

data_path:
  train: ${project_dir}/path_data/path_train.pkl
  val: ${project_dir}/path_data/path_val.pkl
  test: ${project_dir}/path_data/path_test.pkl

pretrained_model_name: klue/roberta-large
num_labels: 7
output_hidden_states: False

metric:
  first_metric: glue
  second_metric: mnli

batch_size: 48

lr: 0.00002

output_directory: ${project_dir}/checkpoints/klue_roberta_large/
save_predictions: ${project_dir}/preds/unimodal_text/unimodal_text.npy

project_name: MultiModalTransformer-UnimodalText
model_name: ${project_name}-bs${batch_size}-lr${lr}-epoch${epoch}

run_name: ${model_name}-all
work_dir: ${hydra:runtime.cwd}

tags: ${model_name}

epoch: 5
